{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrEyeBot Retinal Image Analysis using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "import imutils\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from shutil import copyfile\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import seaborn as sn\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from pyimagesearch.smallervggnet import SmallerVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6052178338166470222\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14091001710071081632\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4385947606174342578\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15560753152\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1750464572407759439\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levelset(x):\n",
    "    \"\"\" Sets Levels 1-4 to level 1\"\"\"\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)\n",
    "\n",
    "def select_toy_images(image_label,N=-1,images_percent=list()):\n",
    "    \"\"\" Selects number of images from each class. By default it is ALL images\"\"\"\n",
    "    image_list = list()\n",
    "    final_images = pd.DataFrame(columns = image_label.columns)\n",
    "    # We create a toy dataset of 'N' images, maintaining the split of the original \n",
    "    if N==-1:\n",
    "            # We need to pick all the images. No need to sample\n",
    "            # We can ignore the percentage here\n",
    "            final_images = image_label\n",
    "    else:\n",
    "        for level in range(5):\n",
    "        # Get respective number of images in each level\n",
    "            if len(images_percent)==5:\n",
    "                number_of_images = int(images_percent[level]*N/100)\n",
    "            else:\n",
    "                # We have no percentage of images. Setting the default safe percentage\n",
    "                images_percent = [73.6,6.9,15.1,2.4,2]\n",
    "                number_of_images = int(images_percent[level]*N/100)\n",
    "            sample_images = image_label[image_label.level==level].sample(n=number_of_images,axis=0)\n",
    "            frames = [final_images,sample_images]\n",
    "            final_images = pd.concat(frames).reset_index(drop=True)\n",
    "    return final_images \n",
    "\n",
    "def preprocess(image,scale=224):\n",
    "    \"\"\" preprocess the test image and covert to array \"\"\"\n",
    "    inter=cv2.INTER_AREA\n",
    "    (h, w) = image.shape[:2]\n",
    "    dW = 0\n",
    "    dH = 0\n",
    "\n",
    "    width = scale\n",
    "    height = scale\n",
    "\n",
    "    # if the width is smaller than the height, then resize\n",
    "    # along the width (i.e., the smaller dimension) and then\n",
    "    # update the deltas to crop the height to the desired\n",
    "    # dimension\n",
    "    if w < h:\n",
    "        image = imutils.resize(image, width=width,\n",
    "            inter=inter)\n",
    "        dH = int((image.shape[0] - height) / 2.0)\n",
    "\n",
    "    # otherwise, the height is smaller than the width so\n",
    "    # resize along the height and then update the deltas\n",
    "    # crop along the width\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height,\n",
    "            inter=inter)\n",
    "        dW = int((image.shape[1] - width) / 2.0)\n",
    "\n",
    "    # now that our images have been resized, we need to\n",
    "    # re-grab the width and height, followed by performing\n",
    "    # the crop\n",
    "    (h, w) = image.shape[:2]\n",
    "    image = image[dH:h - dH, dW:w - dW]\n",
    "\n",
    "    # finally, resize the image to the provided spatial\n",
    "    # dimensions to ensure our output image is always a fixed\n",
    "    # size\n",
    "\n",
    "    image =  cv2.resize(image, (width, height),\n",
    "        interpolation=inter)\n",
    "    return img_to_array(image)\n",
    "\n",
    "def selector(x):\n",
    "    \"\"\" Function to select the class\"\"\"\n",
    "    if x[0] > x[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16_TL(input_shape,layers_to_skip=None,weights=None,include_top=True):\n",
    "    \"\"\" VGG 16 with Transfer Learning. Using Keras built in function\"\"\"\n",
    "    model = applications.VGG16(weights = weights, include_top=include_top, input_shape = input_shape)\n",
    "    if layers_to_skip:\n",
    "        if weights==None:\n",
    "            print(\"ERROR: You cannot have weights as none if layers_to_skip is non-zero\")\n",
    "        else:\n",
    "            for layer in model.layers[:layers_to_skip]:\n",
    "                layer.trainable = False\n",
    "            #Adding custom Layers \n",
    "            x = model.output\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(4096, activation=\"relu\")(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            x = Dense(4096, activation=\"relu\")(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "    else:\n",
    "        x = model.output\n",
    "    \n",
    "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    adam_opt = Adam(lr=0.01)\n",
    "    rms_opt = RMSprop(lr=0.01)\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    # creating the final model \n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "    model_final.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    return model_final\n",
    "\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),input_shape=(length,width,depth),activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    adam_opt = Adam(lr=0.01)\n",
    "    rms_opt = RMSprop(lr=0.01)\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def CNN_FF():\n",
    "    \"\"\" CNN with Feed Forward NN \"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Conv2D(32, kernel_size=(5, 5), \n",
    "                                          input_shape=(length,width,depth),activation='relu'))\n",
    "    model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_conv.add(Flatten())\n",
    "    model_conv.add(Dense(100))\n",
    "    model_conv.add(Dropout(0.1))\n",
    "    model_conv.add(Dense(num_classes, activation='softmax'))\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    model_conv.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    #print(model_conv.summary())\n",
    "    return model_conv\n",
    "\n",
    "\n",
    "def save_model(model,vgg=True):\n",
    "    # saving model\n",
    "    json_model = model.to_json()\n",
    "    # Get today's date. We will use this as string for filename\n",
    "    now = datetime.datetime.now()\n",
    "    day = str(now)[:10]\n",
    "    if vgg:\n",
    "        arch_name = 'model_architecture_vgg_'+day+'35K_aug'+'.json'\n",
    "        model_weights = 'model_weights_vgg_'+day+'35K_aug'+'.h5'\n",
    "    else:\n",
    "        arch_name = 'model_architecture_s_cnn'+day+'.json'\n",
    "        model_weights = 'model_weights_s_cnn'+day+'.h5'\n",
    "        \n",
    "    open(arch_name, 'w').write(json_model)\n",
    "    # saving weights\n",
    "    model.save_weights(model_weights, overwrite=True)\n",
    "\n",
    "def load_model():\n",
    "    # loading model\n",
    "    model = model_from_json(open('model_architecture.json').read())\n",
    "    model.load_weights('model_weights.h5')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the csv data\n",
    "orig_label = pd.read_csv(\"./Retinal-Images/trainLabels.csv\")\n",
    "# Load the test csv data\n",
    "test_label = pd.read_csv(\"./Retinal-Images/retinopathy_solution.csv\")\n",
    "test_label.drop('Usage',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image    35126\n",
       "level    35126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_label.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image    53576\n",
       "level    53576\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image\n",
       "level       \n",
       "0      25810\n",
       "1       2443\n",
       "2       5292\n",
       "3        873\n",
       "4        708"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_label.groupby(['level']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image\n",
       "level       \n",
       "0      39533\n",
       "1       3762\n",
       "2       7861\n",
       "3       1214\n",
       "4       1206"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.groupby(['level']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SAMPLE = 35000\n",
    "images_percent = [73.6,6.9,15.1,2.4,2]\n",
    "toy_images = select_toy_images(orig_label,N=IMAGE_SAMPLE,images_percent=images_percent) \n",
    "images_list = list(toy_images.image)\n",
    "# 80% is trained and 20% is validation\n",
    "n_train = int(IMAGE_SAMPLE*0.8)\n",
    "n_val = IMAGE_SAMPLE-n_train\n",
    "train_images = images_list[:n_train]\n",
    "val_images = images_list[n_train:]\n",
    "# Change the level to 0 or 1 for toy set\n",
    "toy_images.level = toy_images.level.apply(levelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 7000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train,n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SAMPLE = 5000\n",
    "images_percent = [50,5,10,15,20]\n",
    "toy_test_images = select_toy_images(test_label,N=IMAGE_SAMPLE,images_percent=images_percent) \n",
    "test_images_list = list(toy_test_images.image)\n",
    "toy_test_images.level = toy_test_images.level.apply(levelset)\n",
    "toy_test_images.groupby(['level']).count()\n",
    "test_labels = toy_test_images.level.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train images\n",
    "for i in range(len(images_list)):\n",
    "    image_class = toy_images.iloc[[i],[1]].values[0][0]\n",
    "    image_name = toy_images.iloc[[i],[0]].values[0][0]\n",
    "    image_dir = os.getcwd()+'/Retinal-Images/train_resize_224/'\n",
    "    image_loc = image_dir+image_name+'.jpeg'\n",
    "    class_dir = 'class_'+str(image_class)\n",
    "    if image_name in train_images:              \n",
    "        out_dir = os.getcwd()+'/Retinal-Images/train/'+class_dir+'/'\n",
    "        copy_loc = out_dir+image_name+'.jpeg'\n",
    "    else:\n",
    "        out_dir = os.getcwd()+'/Retinal-Images/val/'+class_dir+'/'\n",
    "        copy_loc = out_dir+image_name+'.jpeg'\n",
    "               \n",
    "    copyfile(image_loc,copy_loc)\n",
    "\n",
    "# Copy test images\n",
    "for i in range(len(test_images_list)):\n",
    "    image_class = toy_test_images.iloc[[i],[1]].values[0][0]\n",
    "    image_name = toy_test_images.iloc[[i],[0]].values[0][0]\n",
    "    image_dir = os.getcwd()+'/Retinal-Images/test_resize_224/'\n",
    "    image_loc = image_dir+image_name+'.jpeg'\n",
    "    class_dir = 'class_'+str(image_class)\n",
    "    out_dir = os.getcwd()+'/Retinal-Images/test/test/'\n",
    "    copy_loc = out_dir+image_name+'.jpeg'               \n",
    "    copyfile(image_loc,copy_loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "\n",
    "length = 224\n",
    "width = 224\n",
    "depth = 3\n",
    "num_classes = 2\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "batch_size = 10\n",
    "steps_per_epoch = int(n_train/batch_size)\n",
    "validation_steps = int(n_val/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 700)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch,validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 1 classes.\n",
      "Found 28000 images belonging to 2 classes.\n",
      "Found 7000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "         'Retinal-Images/test',\n",
    "         target_size=(224, 224),\n",
    "         batch_size=10)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'Retinal-Images/train',\n",
    "        target_size=(224,224),\n",
    "        classes = ['class_0','class_1'],\n",
    "        batch_size=10,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "         'Retinal-Images/val',\n",
    "         target_size=(224, 224),\n",
    "         classes = ['class_0','class_1'],\n",
    "         batch_size=10,\n",
    "         class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_0': 0, 'class_1': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54347826, 6.25      ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(train_generator.classes), \n",
    "            train_generator.classes)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 138,359,546\n",
      "Trainable params: 138,359,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  import sys\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_steps=700, steps_per_epoch=2800, class_weight=array, validation_data=<keras_pre..., epochs=10)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2800/2800 [==============================] - 394s 141ms/step - loss: 0.3830 - acc: 0.9195 - val_loss: 1.9669 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2800/2800 [==============================] - 382s 137ms/step - loss: 0.2858 - acc: 0.9200 - val_loss: 2.3117 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2800/2800 [==============================] - 383s 137ms/step - loss: 0.2798 - acc: 0.9200 - val_loss: 2.4405 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2800/2800 [==============================] - 380s 136ms/step - loss: 0.2789 - acc: 0.9200 - val_loss: 2.4891 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2800/2800 [==============================] - 383s 137ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5054 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2800/2800 [==============================] - 381s 136ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5204 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2800/2800 [==============================] - 386s 138ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5241 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2800/2800 [==============================] - 385s 138ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5260 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2800/2800 [==============================] - 385s 137ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5274 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2800/2800 [==============================] - 384s 137ms/step - loss: 0.2788 - acc: 0.9200 - val_loss: 2.5257 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1b65aa978>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG_16_TL(input_shape)\n",
    "print(model.summary())\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        nb_epoch=10,steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIME FOR PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step\n",
      "TN: 2500,FP: 0, FN: 2500, TP:0\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict_generator(test_generator,verbose=1,steps=500)\n",
    "prediction_vgg = np.array([selector(xi) for xi in pred])\n",
    "cnf = confusion_matrix(test_labels, prediction_vgg)\n",
    "tn, fp, fn, tp = cnf.ravel()\n",
    "print(\"TN: {},FP: {}, FN: {}, TP:{}\".format(tn,fp,fn,tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb1b5f78438>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm = pd.DataFrame(cnf, index = [i for i in range(2)],\n",
    "                  columns = [i for i in range(2)])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"YlGnBu\",fmt='g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
